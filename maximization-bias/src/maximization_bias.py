import numpy as np
from zmq.backend import first

# region Hyper-parameters

# States
states = dict(A = 0, B = 1, terminal = 2)

# Start state
start = states['A']

# Possible actions from state A
actions_A = dict(right = 0, left = 1)

# Possible actions from state B (e.g., 10 actions (will affect the curves))
actions_B = range(0, 10)

# All possible actions
actions = [[actions_A["right"], actions_A["left"]], actions_B]

# State-action pair values. The value of a terminal state is always 0
state_action_values = [np.zeros(2), np.zeros(len(actions_B)), np.zeros(1)]

# Destination for each state and each action
transition = [[states["terminal"], states['B']], [states["terminal"]] * len(actions_B)]

# Exploration probability (denoted as ğœ€)
exploration_probability = 0.1

# Step-size parameter (denoted as ğ›¼)
step_size = 0.1

# Discount rate for max value (denoted as ğ›¾)
discount = 1.0

# endregion Hyper-parameters

# region Functions

def choose_action(action_value_estimates, state):
    # region Summary
    """
    Chooses an action based on ğœ€-greedy algorithm
    :param action_value_estimates: Action-value estimates (denoted as ğ‘„(ğ‘†_ğ‘¡, ğ´_ğ‘¡))
    :param state: State
    :return: Action
    """
    # endregion Summary

    # region Body

    # Îµ-greedy action selection: every once in a while, with small probability Îµ, select randomly from among all the actions with equal probability, independently of the action-value estimates.
    if np.random.binomial(n=1, p=exploration_probability) == 1:
        action = np.random.choice(actions[state])

    # Greedy action selection: select one of the actions with the highest estimated value, that is, one of the greedy actions.
    # If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly.
    else:
        values = action_value_estimates[state]
        action = np.random.choice([act for act, val in enumerate(values) if val == np.max(values)])

    return action

    # endregion Body

def take_action(state, action):
    # region Summary
    """
    Takes an action in state and gets reward
    :param state: State
    :param action: Action
    :return: Reward
    """
    # endregion Summary

    # region Body

    # The reward from state A is 0 regardless of the action
    if state == states['A']:
        return 0

    # The reward from state B is drawn from a normal distribution with ğœ‡ = -0.1 mean and ğœ = 1.0 variance for all possible actions
    return np.random.normal(-0.1, 1)

    # endregion Body

def q_learning(first_action_value_estimates, second_action_value_estimates = None):
    # region Summary
    """
    Counts the number of "left" action for Q-learning or Double Q-learning
    :param first_action_value_estimates: 1st action-value estimates (denoted as ğ‘„_1(ğ‘))
    :param second_action_value_estimates: 2nd action-value estimates (denoted as ğ‘„_2(ğ‘)). If not None, then this function is Double Q-learning; otherwise, it is classic Q-learning
    :return: Number of "left" action in state A
    """
    # endregion Summary

    # region Body

    # Initialize state at the start
    state = start


    # Track the number of action "left" in state A
    left_count = 0

    # Keep going until getting to the terminal state
    while state != states['terminal']:
        # choose an action for classic Q-learning
        if second_action_value_estimates is None:
            action = choose_action(first_action_value_estimates, state)

        # choose an action for Double Q-learning
        else:
            # for example, an ğœ€-greedy policy for Double Q-learning could be based on the average (or sum) of the 2 action-value estimates
            action = choose_action([estimate1 + estimate2 for estimate1, estimate2 in zip(first_action_value_estimates, second_action_value_estimates)], state)

        # check if agent chose "left" action in state A
        if state == states['A'] and action == actions_A["left"]:
            left_count += 1

        # get the reward
        reward = take_action(state, action)

        # get the next state
        next_state = transition[state][action]
        # for classic Q-learning
        if second_action_value_estimates is None:
            # set action-value estimate to update
            update_estimate = first_action_value_estimates

            # set target
            target = np.max(update_estimate[next_state])

        # for Double Q-learning, divide the time steps in 2, perhaps by flipping a coin on each step
        else:
            # if the coin comes up heads
            if np.random.binomial(n=1, p=0.5) == 1:
                # set the estimate to update to ğ‘„_1
                update_estimate = first_action_value_estimates

                # set the target estimate to ğ‘„_2
                target_estimate = second_action_value_estimates

            # if the coin comes up tails, then the same update is done with ğ‘„_1 and ğ‘„_2 switched, so that ğ‘„_2 is updated
            else:
                # set the estimate to update to ğ‘„_2
                update_estimate = second_action_value_estimates

                # set the target estimate to ğ‘„_1
                target_estimate = first_action_value_estimates

            # get the best action
            best_action = np.random.choice([act for act , val in enumerate(update_estimate[next_state]) if val == np.max(update_estimate[next_state])])
            # get the target
            target = target_estimate[next_state][best_action]

        # Q-learning update (Equation (6.8))
        update_estimate[state][action] += step_size * (reward  + discount * target - update_estimate[state][action])

        # move to the next state
        state = next_state

    return left_count

    # endregion Body

# endregion Functions