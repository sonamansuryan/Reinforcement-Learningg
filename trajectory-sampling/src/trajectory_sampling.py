import numpy as np
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp


# region Hyper-parameters

# 2 possible actions
actions = [0, 1]

# Each transition has a probability to terminate with 0
termination_probability = 0.1

# Maximum expected updates
max_steps = 20000

# Exploration probability for behavior policy
exploration_probability = 0.1

# endregion Hyper-parameters

# region Functions

# region Helpers

def argmax(action_value_estimates):
    # region Summary
    """
    Break tie randomly when selecting action with maximum Q-value
    :param action_value_estimates: Action-value estimates
    :return: Action with maximum estimated value
    """
    # endregion Summary

    # region Body

    # Get the maximum of action-value estimates
    max_estimate = np.max(action_value_estimates)

    # Get the action with maximum action-value estimate
    action = np.random.choice([action_ for action_, action_value_estimate in enumerate(action_value_estimates) if action_value_estimate == max_estimate])

    return action

    # endregion Body

class Task:
    # region Constructor

    def __init__(self, n_states, branching_factor):
        # region Summary
        """
        MDP Task with random transitions and rewards
        :param n_states: Number of non-terminal states
        :param branching_factor: Branching factor (number of possible next states per state-action pair)
        """
        # endregion Summary

        # region Body

        self.n_states = n_states
        self.branching_factor = branching_factor

        # Pre-compute transition matrix and rewards for all state-action pairs
        self.transition = np.random.randint(n_states, size=(n_states, len(actions), branching_factor))
        self.reward = np.random.randn(n_states, len(actions), branching_factor)

        # Pre-compute termination mask for vectorized operations
        self.termination_mask = np.random.rand(max_steps) < termination_probability
        self.termination_idx = 0

        # Pre-compute random choices for next state selection
        self.random_choices = np.random.randint(branching_factor, size=max_steps)
        self.choice_idx = 0

        # endregion Body

    # endregion Constructor

    # region Functions

    def step(self, state, action):
        # region Summary
        """
        Take a step in the environment
        :param state: Current state
        :param action: Action
        :return: Next state (n_states if terminal) and reward for this transition
        """
        # endregion Summary

        # region Body

        # Check termination using pre-computed values
        if self.termination_mask[self.termination_idx % max_steps]:
            self.termination_idx += 1
            return self.n_states, 0.0

        # Select next state using pre-computed random choice
        next_idx = self.random_choices[self.choice_idx % max_steps]
        self.choice_idx += 1
        self.termination_idx += 1

        # Get the next state
        next_state = self.transition[state, action, next_idx]

        # Get the reward
        reward = self.reward[state, action, next_idx]

        return next_state, reward

        # endregion Body

    # endregion Functions

def evaluate_policy(state_action_value_estimates, task, runs=1000):
    # region Summary
    """
    Evaluate the value of start state under greedy policy using Monte Carlo
    :param state_action_value_estimates: State-action value estimates
    :param task: MDP task
    :param runs: Number of MC runs
    :return: Average return from start state
    """
    # endregion Summary

    # region Body

    # Create an array for returns filled with 0s
    returns = np.zeros(runs)

    # For every run
    for run in range(runs):
        # initialize total reward
        total_reward = 0.0

        # track the number of states
        state = 0

        # track the number of steps
        steps = 0

        # while there are states in task and step limit is not exceeded
        while state < task.n_states and steps < 1000:
            # get the action with maximum estimated value
            action = argmax(state_action_value_estimates[state])

            # get the next state and reward
            next_state, reward = task.step(state, action)

            # add the reward to total rewards
            total_reward += reward

            # update state for next iteration
            state = next_state

            # increment the number of steps
            steps += 1

        # add the total reward to the returns for current run
        returns[run] = total_reward

    return np.mean(returns)

    # endregion Body

# endregion Helpers

# region Sampling

def uniform_sampling(task, eval_interval):
    # region Summary
    """
    Perform expected updates with uniform state-action distribution
    :param task: MDP task
    :param eval_interval: Steps between evaluations
    :return: Tuple of (steps, performance) arrays
    """
    # endregion Summary

    # region Body

    # Create an empty list for performance


    # Create a matrix for state-action value estimates filled with 0s


    # Pre-compute evaluation steps


    # For every step

        # get the current state


        # get the action


        # get the next states


        # get the rewards


        # get the next value estimates


        # update estimates


        # check if it's time to evaluate

            # get the average return


            # append the step and average return to the performance list


            # increment the evaluation index




    # endregion Body


def on_policy_sampling(task, eval_interval):
    # region Summary
    """
    Perform expected updates with on-policy distribution
    :param task: MDP task
    :param eval_interval: Steps between evaluations
    :return: Tuple of (steps, performance) arrays
    """
    # endregion Summary

    # region Body

    # Create an empty list for performance


    # Create a matrix for state-action value estimates filled with 0s


    # Initialize state counter to 0


    # Pre-compute evaluation steps


    # Pre-compute ε-greedy decisions


    # Get random actions


    # For every step

        # ε-greedy action selection

            # choose random action


            # choose action with maximum estimated value


        # get the next state


        # get the next states


        # get the rewards


        # get the next value estimates


        # update estimates


        # reset to start state if terminal


        # check if it's time to evaluate

            # get the average return


            # append the step and average return to the performance list


            # increment the evaluation index




    # endregion Body

# endregion Sampling

# region Runs

def run_single_task(args):
    # region Summary
    """
    Run a single task for parallel processing
    :param args: Tuple of (task, method, eval_interval)
    :return: Performance data for this task
    """
    # endregion Summary

    # region Body

    task, method, eval_interval = args

    return method(task, eval_interval)

    # endregion Body


def run_experiment(n_states, branching_factor, methods, n_tasks=30, evaluation_points=100):
    # region Summary
    """
    Run the complete experiment for given parameters
    :param n_states: Number of states
    :param branching_factor: Branching factor
    :param methods: List of methods to test
    :param n_tasks: Number of tasks to average over
    :param evaluation_points: Number of evaluation points
    :return: Dictionary with results for each method
    """
    # endregion Summary

    # region Body

    # Get the evaluation interval
    eval_interval = max_steps // evaluation_points

    # Create an empty dictionary for results
    results = {}

    # Generate all tasks upfront
    tasks = [Task(n_states, branching_factor) for _ in range(n_tasks)]

    # For every method
    for method in methods:
        print(f"Running {method.__name__} for {n_states} states, b={branching_factor}")

        # prepare arguments for parallel processing
        args_list = [(task, method, eval_interval) for task in tasks]

        # use parallel processing for better performance
        with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:
            task_results = list(executor.map(run_single_task, args_list))

        # aggregate results
        all_steps = None
        all_values = []

        # get the task results
        for steps, values in task_results:
            if all_steps is None:
                all_steps = np.array(steps)
            all_values.append(values)

        # calculate mean performance across tasks
        mean_values = np.mean(np.array(all_values), axis=0)
        results[method.__name__] = (all_steps, mean_values)

    return results

    # endregion Body

# endregion Runs

# endregion Functions
