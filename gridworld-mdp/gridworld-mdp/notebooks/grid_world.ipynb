{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gridworld as Finite MDP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42f01993af4e5d74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policies and Value Functions\n",
    "Figure 3.2 (left) shows a rectangular (square) gridworld representation of a simple finite MDP.\n",
    "- The cells of the grid correspond to the states of the environment.\n",
    "- At each cell, 4 actions are possible: north (up), south (down), east (right), and west (left), which deterministically cause the agent to move 1 cell in the respective direction on the grid.\n",
    "- Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1.\n",
    "- Other actions result in a reward of 0, except those that move the agent out of the special states A and B.\n",
    "    - From state A, all 4 actions yield a reward of +10 and take the agent to A'.\n",
    "    - From state B, all 4 actions yield a reward of +5 and take the agent to B'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34526935c9d4ba95"
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import Image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd597829053dc8e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "Image(filename=\"../book_images/Figure_3_2.PNG\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b553596613d7b402",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Suppose the agent selects all 4 actions with equal probability in all states => the probability of each action will be 1/4.\n",
    "action_probability = 0.25\n",
    "\n",
    "# Discount rate (denoted as 0 ‚â§ ùõæ ‚â§ 1)\n",
    "discount = 0.9"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f6c128f07877285",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Figure 3.2 (right) shows the value function, $v_{\\pi}$, for this policy, for the discounted reward case with $\\gamma$ = 0.9. This VF was computed by solving the system of linear equations (3.14), i.e. Bellman equation for $v_{\\pi}$.\n",
    "- Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy.\n",
    "- State A is the best state to be in under this policy.\n",
    "- Note that A‚Äôs expected return is less than its immediate reward of 10, because from A state the agent is taken to state A' from which it is likely to run into the edge of the grid.\n",
    "- State B, on the other hand, is valued more than its immediate reward of 5, because from B the agent is taken to B' which has a positive value.\n",
    "- From B' the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d837b84a9dc7563a"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.grid_world import grid_size, actions, step, draw\n",
    "\n",
    "matplotlib.use('Agg')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7d3c95f4455c2e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# State-value function table\n",
    "state_values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Iterate until value convergence\n",
    "while True:\n",
    "    # New values of state-value function table\n",
    "    new_values = np.zeros_like(state_values)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            for action in actions:\n",
    "                # Get the current state\n",
    "                state = [i, j]\n",
    "                \n",
    "                # Get the next state and reward\n",
    "                next_state, reward = step(state, action)\n",
    "                \n",
    "                # Bellman equation for ùë£_ùúã\n",
    "                new_values[i, j] += action_probability * (reward + discount * state_values[next_state[0], next_state[1]])\n",
    "    \n",
    "    # Check value convergence\n",
    "    if np.sum(np.abs(state_values - new_values)) < 1e-4:\n",
    "        draw(grid=np.round(new_values, decimals=2))\n",
    "        plt.savefig(\"../generated_images/figure_3_2.png\")\n",
    "        plt.close()\n",
    "        break\n",
    "    \n",
    "    state_values = new_values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimal Policies and Optimal Value Functions\n",
    "Suppose we solve the Bellman equation for $v_{*}$ for the simple grid task shown in Figure 3.5 (left).\n",
    "- Figure 3.5 (middle) shows the optimal value function.\n",
    "- Figure 3.5 (right) shows the corresponding optimal policies.\n",
    "- Where there are multiple arrows in a cell, all the corresponding actions are optimal."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a0e4eb7c9602183"
  },
  {
   "cell_type": "code",
   "source": [
    "Image(filename=\"../book_images/Figure_3_5.PNG\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c828f277603291",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# State-value function table\n",
    "state_values = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Iterate until value convergence\n",
    "while True:\n",
    "    # New values of state-value function table\n",
    "    new_values = np.zeros_like(state_values)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            # Create an empty list of state-values\n",
    "            values = []\n",
    "            \n",
    "            for action in actions:\n",
    "                # Get the current state\n",
    "                state = [i, j]\n",
    "                \n",
    "                # Get the next state and reward\n",
    "                next_state, reward = step(state, action)\n",
    "                \n",
    "                # Bellman equation for ùíó_‚àó or Bellman optimality equation\n",
    "                values.append(reward + discount * state_values[next_state[0], next_state[1]])\n",
    "            \n",
    "            # Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy\n",
    "            # must equal the expected return for the best action from that state:\n",
    "            new_values[i, j] = np.max(values)\n",
    "    \n",
    "    # Check value convergence\n",
    "    if np.sum(np.abs(new_values - state_values)) < 1e-4:\n",
    "        draw(grid=np.round(new_values, decimals=2))\n",
    "        plt.savefig(\"../generated_images/figure_3_5.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        draw(grid=new_values, is_policy=True)\n",
    "        plt.savefig(\"../generated_images/figure_3_5_policy.png\")\n",
    "        plt.close()\n",
    "        break\n",
    "    \n",
    "    state_values = new_values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5946f822944ab776",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
